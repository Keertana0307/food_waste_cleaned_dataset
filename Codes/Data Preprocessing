import pandas as pd
import numpy as np
import os
import gdown

# ================================
# 0. DOWNLOAD FILE FROM GOOGLE DRIVE
# ================================
file_id = "12IX9yDyfryXAUoOXcmQLvRWwZrfX10KF"  # From your shared file link
filename = "raw_food_waste.csv"

if not os.path.exists(filename):
    print(f"‚¨áÔ∏è Downloading '{filename}' from Google Drive...")
    url = f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, filename, quiet=False)
else:
    print(f"‚úÖ '{filename}' already exists locally. Skipping download.")

# ==========================================
# 1. SMART HEADER SEARCH
# ==========================================
print("üîç Searching for the correct header row...")

# Read first 20 rows to find header
try:
    peek_df = pd.read_csv(filename, header=None, nrows=20)
except Exception as e:
    print(f"‚ùå Error reading file: {e}")
    exit()

header_row_index = -1
for i, row in peek_df.iterrows():
    row_values = [str(val).lower() for val in row.values]
    if "year" in row_values and "state" in row_values:
        header_row_index = i
        print(f"‚úÖ Found real headers at Row {i}")
        break

if header_row_index == -1:
    print("‚ùå CRITICAL ERROR: Could not find 'Year' or 'State' in the first 20 rows.")
    exit()

# ==========================================
# 2. LOAD & CLEAN
# ==========================================
df = pd.read_csv(filename, header=header_row_index)

# Standardize Columns
df.columns = (
    df.columns
    .str.strip()
    .str.lower()
    .str.replace(" ", "_")
    .str.replace("-", "_")
    .str.replace(r"\(.*\)", "", regex=True)
    .str.replace(".", "")
)

# Select Features
target = "tons_waste"
features = [
    "year", "state", "sector", "sub_sector", "food_type",
    "tons_surplus", "tons_supply", "tons_uneaten",
    "surplus_total_100_year_mtco2e_footprint",
    "surplus_total_100_year_mtch4_footprint",
    "gallons_water_footprint", "meals_wasted"
]

available_cols = [col for col in features + [target] if col in df.columns]
df = df[available_cols]

# Force Numeric
numeric_cols = [
    "tons_waste", "tons_surplus", "tons_supply", "tons_uneaten",
    "surplus_total_100_year_mtco2e_footprint",
    "surplus_total_100_year_mtch4_footprint",
    "gallons_water_footprint", "meals_wasted", "year"
]
for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Drop Missing (NaN)
df = df.dropna()

# ==========================================
# 3. REMOVE ZERO VALUES
# ==========================================
print(f"\n--- Removing Zero Values ---")
rows_before = len(df)

zero_check_cols = [
    "tons_waste", "tons_surplus", "tons_supply", "tons_uneaten",
    "surplus_total_100_year_mtco2e_footprint",
    "surplus_total_100_year_mtch4_footprint",
    "gallons_water_footprint", "meals_wasted"
]
cols_to_filter = [col for col in zero_check_cols if col in df.columns]

# Drop row if ANY of these columns is 0
df = df[(df[cols_to_filter] != 0).all(axis=1)]

rows_after = len(df)
print(f"Rows before zero-filtering: {rows_before}")
print(f"Rows after zero-filtering:  {rows_after}")
print(f"‚ùå Removed {rows_before - rows_after} rows containing zeros.")

# ==========================================
# 4. SAVE CLEANED DATASET
# ==========================================
if rows_after == 0:
    print("\n‚ö†Ô∏è WARNING: Dataset is empty! Check your filter logic.")
else:
    output_file = "cleaned_food_waste_dataset.csv"
    df.to_csv(output_file, index=False)
    print(f"\nüéâ SUCCESS! Saved '{output_file}' with {rows_after} rows.")
    print(df.head())
