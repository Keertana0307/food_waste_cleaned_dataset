# ==========================================
# IMPORT LIBRARIES
# ==========================================
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# ==========================================
# 1. LOAD CLEANED DATASET (GITHUB)
# ==========================================
url = "https://raw.githubusercontent.com/Keertana0307/food_waste_cleaned_dataset/main/cleaned_food_waste_dataset.xlsx"
df = pd.read_excel(url)

# Standardize column names
df.columns = (
    df.columns.str.strip()
    .str.lower()
    .str.replace(" ", "_")
    .str.replace("-", "_")
)

print("âœ… Dataset loaded:", df.shape)

# ==========================================
# 2. REMOVE EXTREME OUTLIERS (1%â€“99%)
# ==========================================
numeric_cols = [
    "tons_waste", "tons_surplus", "tons_supply", "tons_uneaten",
    "surplus_total_100_year_mtco2e_footprint",
    "surplus_total_100_year_mtch4_footprint",
    "gallons_water_footprint", "meals_wasted", "year"
]

for col in numeric_cols:
    if col in df.columns:
        lower = df[col].quantile(0.01)
        upper = df[col].quantile(0.99)
        df = df[(df[col] >= lower) & (df[col] <= upper)]

print("âœ… Extreme outliers removed using 1st/99th percentile filtering.")
print("Dataset after outlier removal:", df.shape)

# ==========================================
# 3. LOG-TRANSFORM TARGET (NN ONLY)
# ==========================================
target = "tons_waste"
df[target + "_log"] = np.log1p(df[target])

print("âœ… Log-transform applied to target.")

# ==========================================
# 4. FEATURES & TARGET
# ==========================================
X = df.drop(columns=[target, target + "_log"])
y_log = df[target + "_log"]        # used for training
y_actual = df[target]              # used for evaluation

categorical_features = ["state", "sector", "sub_sector", "food_type"]
numerical_features = [col for col in X.columns if col not in categorical_features]

# ==========================================
# 5. PREPROCESSOR
# ==========================================
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numerical_features),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features),
    ]
)

# ==========================================
# 6. FNN MODEL FUNCTION
# ==========================================
def build_fnn(input_dim):
    model = Sequential([
        Dense(128, activation="relu", input_dim=input_dim),
        Dropout(0.2),
        Dense(64, activation="relu"),
        Dropout(0.2),
        Dense(1)
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss="mse",
        metrics=["mae"]
    )
    return model

# ==========================================
# 7. 5-FOLD CROSS-VALIDATION
# ==========================================
kf = KFold(n_splits=5, shuffle=True, random_state=42)

mae_list, rmse_list, r2_list = [], [], []

for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):
    print(f"\nðŸ” Fold {fold}")

    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train_log = y_log.iloc[train_idx]
    y_test_actual = y_actual.iloc[test_idx]

    # Preprocess
    X_train_p = preprocessor.fit_transform(X_train)
    X_test_p = preprocessor.transform(X_test)

    # Build model
    model = build_fnn(X_train_p.shape[1])

    early_stop = EarlyStopping(
        monitor="val_loss",
        patience=5,
        restore_best_weights=True
    )

    # Train
    model.fit(
        X_train_p,
        y_train_log,
        validation_split=0.1,
        epochs=100,
        batch_size=256,
        callbacks=[early_stop],
        verbose=0
    )

    # Predict (log scale â†’ inverse)
    y_pred_log = model.predict(X_test_p).flatten()
    y_pred_actual = np.expm1(y_pred_log)

    # Metrics (original scale)
    mae = mean_absolute_error(y_test_actual, y_pred_actual)
    rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_actual))
    r2 = r2_score(y_test_actual, y_pred_actual)

    mae_list.append(mae)
    rmse_list.append(rmse)
    r2_list.append(r2)

# ==========================================
# 8. FINAL RESULTS
# ==========================================
print("\nâœ… FNN 5-Fold Cross-Validation Results (Outliers Removed + Log Target)")
print(f"Average MAE : {np.mean(mae_list):,.2f}")
print(f"Average RMSE: {np.mean(rmse_list):,.2f}")
print(f"Average RÂ²  : {np.mean(r2_list):.4f}")
