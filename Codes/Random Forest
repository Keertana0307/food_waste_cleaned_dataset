# ==========================================
# IMPORT LIBRARIES
# ==========================================
import numpy as np
import pandas as pd

from sklearn.model_selection import KFold
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline


# ==========================================
# 1. LOAD CLEANED DATASET (GITHUB)
# ==========================================
url = "https://raw.githubusercontent.com/Keertana0307/food_waste_cleaned_dataset/main/cleaned_food_waste_dataset.xlsx"
df = pd.read_excel(url)

# Standardize column names
df.columns = (
    df.columns.str.strip()
    .str.lower()
    .str.replace(" ", "_")
    .str.replace("-", "_")
)

print("âœ… Dataset loaded:", df.shape)


# ==========================================
# 2. REMOVE EXTREME OUTLIERS (1%â€“99%)
# ==========================================
numeric_cols = [
    "tons_waste", "tons_surplus", "tons_supply", "tons_uneaten",
    "surplus_total_100_year_mtco2e_footprint",
    "surplus_total_100_year_mtch4_footprint",
    "gallons_water_footprint", "meals_wasted", "year"
]

for col in numeric_cols:
    if col in df.columns:
        lower = df[col].quantile(0.01)
        upper = df[col].quantile(0.99)
        df = df[(df[col] >= lower) & (df[col] <= upper)]

print("âœ… Extreme outliers removed using 1st/99th percentile filtering.")
print("Dataset after outlier removal:", df.shape)


# ==========================================
# 3. FEATURES & TARGET
# ==========================================
target = "tons_waste"

features = [
    "year",
    "state",
    "sector",
    "sub_sector",
    "food_type",
    "tons_surplus",
    "tons_supply",
    "tons_uneaten",
    "surplus_total_100_year_mtco2e_footprint",
    "surplus_total_100_year_mtch4_footprint",
    "gallons_water_footprint",
    "meals_wasted"
]

X = df[features]
y = df[target]

categorical_features = ["state", "sector", "sub_sector", "food_type"]
numerical_features = [col for col in features if col not in categorical_features]


# ==========================================
# 4. PREPROCESSOR (NO SCALING NEEDED FOR RF)
# ==========================================
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ("num", "passthrough", numerical_features)
    ]
)


# ==========================================
# 5. RANDOM FOREST PIPELINE
# ==========================================
rf_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", RandomForestRegressor(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1
    ))
])

# ==========================================
# 6. STORE FEATURE IMPORTANCE ACROSS FOLDS
# ==========================================
feature_importance_list = []
feature_names = None


# # ==========================================
# 7. 5-FOLD CROSS-VALIDATION + FEATURE IMPORTANCE
# ==========================================
kf = KFold(n_splits=5, shuffle=True, random_state=42)

mae_list, rmse_list, r2_list = [], [], []

for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):
    print(f"\nðŸ” Fold {fold}")

    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    # Train model
    rf_pipeline.fit(X_train, y_train)

    # Predict
    y_pred = rf_pipeline.predict(X_test)

    # Metrics
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    mae_list.append(mae)
    rmse_list.append(rmse)
    r2_list.append(r2)

    # ===============================
    # FEATURE IMPORTANCE
    # ===============================
    rf_model = rf_pipeline.named_steps["model"]
    preprocessor = rf_pipeline.named_steps["preprocessor"]

    # Get feature names after encoding
    fold_feature_names = preprocessor.get_feature_names_out()

    if feature_names is None:
        feature_names = fold_feature_names

    feature_importance_list.append(rf_model.feature_importances_)

# ==========================================
# 8. AGGREGATED FEATURE IMPORTANCE
# ==========================================
avg_importance = np.mean(feature_importance_list, axis=0)

feature_importance_df = pd.DataFrame({
    "feature": feature_names,
    "importance": avg_importance
}).sort_values(by="importance", ascending=False)

print("\nâœ… Top 15 Most Important Features (Averaged Across Folds)")
print(feature_importance_df.head(15))


# ==========================================
# 9. FINAL RESULTS
# ==========================================
print("\nâœ… Random Forest Performance Results (5-Fold CV):")
print(f"Average MAE : {np.mean(mae_list):,.2f}")
print(f"Average RMSE: {np.mean(rmse_list):,.2f}")
print(f"Average RÂ²  : {np.mean(r2_list):.4f}")

# ==========================================
# FEATURE IMPORTANCE REPORT
# ==========================================
import matplotlib.pyplot as plt

# Build DataFrame for feature importance
feature_importance_df = pd.DataFrame({
    "feature": feature_names,
    "importance": avg_importance
}).sort_values(by="importance", ascending=False)

# Display full table with decimals
pd.set_option("display.float_format", "{:.5f}".format)
print("\nFeature Importance (Random Forest):")
print(feature_importance_df)

# Top 10 Features
top10_features = feature_importance_df.head(10)
print("\nTop 10 Feature Importances (Random Forest):")
print(top10_features)

# Plot Top 10 Features
plt.figure(figsize=(10,6))
plt.barh(top10_features["feature"][::-1], top10_features["importance"][::-1], color="skyblue")
plt.xlabel("Feature Importance")
plt.title("Top 10 Feature Importances (Random Forest)")
plt.show()

# Plot 5 Least Important Features
bottom5_features = feature_importance_df.tail(5)
plt.figure(figsize=(10,6))
plt.barh(bottom5_features["feature"], bottom5_features["importance"], color="salmon")
plt.xlabel("Feature Importance")
plt.title("5 Least Important Features (Random Forest)")
plt.show()
